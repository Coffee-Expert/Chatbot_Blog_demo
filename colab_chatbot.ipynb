{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# TinyLlama Local Chatbot - Complete Google Colab Implementation\n",
        "\n",
        "This notebook will guide you through building a fully functional AI chatbot that runs locally using the TinyLlama-1.1B model.\n",
        "\n",
        "**What you'll build:**\n",
        "- A conversational AI assistant\n",
        "- Memory-enabled chat system\n",
        "- GPU/CPU automatic optimization\n",
        "- Professional response generation\n",
        "\n",
        "just select a T4 GPU from runtype and click the play button!\n",
        "Let's build your AI chatbot step by step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-dependencies"
      },
      "source": [
        "## Step 1: Install Required Dependencies\n",
        "\n",
        "First, we need to install the essential Python libraries for our chatbot:\n",
        "- **transformers**: Hugging Face library for loading AI models\n",
        "- **torch**: PyTorch for tensor operations and model inference\n",
        "- **accelerate**: Optimizes model loading and GPU utilization\n",
        "\n",
        "The `-q` flag keeps the installation output minimal and clean.\n",
        "\n",
        "**Expected output:** \"Dependencies installed successfully!\" message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install",
        "outputId": "a6adce77-98c0-496b-d7e7-43773983aee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dependencies installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch accelerate -q\n",
        "print(\"Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import-libraries"
      },
      "source": [
        "## Step 2: Import Essential Libraries\n",
        "\n",
        "Now we import all the Python libraries we'll need:\n",
        "- **torch**: For GPU detection and memory management\n",
        "- **transformers**: For loading and running the TinyLlama model\n",
        "- **time**: To measure response times\n",
        "- **gc**: For garbage collection and memory cleanup\n",
        "\n",
        "**What happens:** All necessary modules are loaded into memory for use in subsequent cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import time\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chatbot-class"
      },
      "source": [
        "## Step 3: Define the Main Chatbot Class\n",
        "\n",
        "This creates our main `ColabChatbot` class that handles all chatbot functionality:\n",
        "\n",
        "**Key features:**\n",
        "- **model_name**: Specifies which AI model to use (TinyLlama-1.1B-Chat-v1.0)\n",
        "- **pipe**: Will store our loaded model pipeline --> Workflow of our AI ChatBot\n",
        "- **conversation_history**: Maintains chat context across exchanges --> Memory\n",
        "- **load_model()**: Automatically called during initialization\n",
        "\n",
        "**What happens:** The chatbot object is created and immediately loads the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "class-init"
      },
      "outputs": [],
      "source": [
        "class ColabChatbot:\n",
        "    def __init__(self, model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
        "        self.model_name = model_name\n",
        "        self.pipe = None\n",
        "        self.conversation_history = []\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load TinyLlama model optimized for Colab\"\"\"\n",
        "        print(\"Loading TinyLlama model (this takes 2-3 minutes first time)...\")\n",
        "\n",
        "        try:\n",
        "            # Use GPU if available, fallback to CPU\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "            print(f\"Using device: {device.upper()}\")\n",
        "\n",
        "            # Load with memory optimization\n",
        "            self.pipe = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=self.model_name,\n",
        "                dtype=dtype,\n",
        "                device_map=\"auto\" if device == \"cuda\" else None,\n",
        "                model_kwargs={\"low_cpu_mem_usage\": True}\n",
        "            )\n",
        "\n",
        "            print(\"Model loaded successfully!\")\n",
        "            print(\"Type 'quit' to exit, 'clear' to reset conversation\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            self.pipe = None\n",
        "\n",
        "    def generate_response(self, user_input):\n",
        "        \"\"\"Generate response with conversation context\"\"\"\n",
        "        if not self.pipe:\n",
        "            return \"Model not loaded. Please restart and try again.\"\n",
        "\n",
        "        try:\n",
        "                # Build conversation context\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a helpful AI assistant.\"\n",
        "                }\n",
        "            ]\n",
        "\n",
        "                # Add recent conversation history (last 4 exchanges to save memory)\n",
        "            recent_history = self.conversation_history[-8:] if len(self.conversation_history) > 8 else self.conversation_history\n",
        "            messages.extend(recent_history)\n",
        "\n",
        "                # Add current user input\n",
        "            messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "                # Format for TinyLlama\n",
        "            prompt = self.pipe.tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                    tokenize=False,\n",
        "                    add_generation_prompt=True\n",
        "                )\n",
        "\n",
        "                # Generate response with memory management\n",
        "            with torch.no_grad():\n",
        "                    outputs = self.pipe(\n",
        "                        prompt,\n",
        "                        max_new_tokens=200,  # Shorter for Colab\n",
        "                        do_sample=True,\n",
        "                        temperature=0.7,\n",
        "                        top_k=50,\n",
        "                        top_p=0.9,\n",
        "                        pad_token_id=self.pipe.tokenizer.eos_token_id,\n",
        "                        return_full_text=False\n",
        "                    )\n",
        "\n",
        "            response = outputs[0][\"generated_text\"].strip()\n",
        "\n",
        "                # Clean up response\n",
        "            if not response:\n",
        "                    response = \"I'm not sure how to respond to that. Could you try asking differently?\"\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error generating response: {str(e)[:100]}...\"\n",
        "\n",
        "    def clear_memory(self):\n",
        "        \"\"\"Clear conversation history and GPU cache\"\"\"\n",
        "        self.conversation_history = []\n",
        "        if torch.cuda.is_available():\n",
        "           torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"Conversation cleared!\")\n",
        "\n",
        "    def start_chat(self):\n",
        "        \"\"\"Main chat loop\"\"\"\n",
        "        if not self.pipe:\n",
        "            print(\"Cannot start chat - model not loaded\")\n",
        "            return\n",
        "\n",
        "        print(\"TinyLlama Chatbot Ready!\")\n",
        "        print(\"Ask me anything about business, technology, or general questions.\")\n",
        "        print()\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                # Get user input\n",
        "                user_input = input(\"You: \").strip()\n",
        "\n",
        "                # Handle special commands\n",
        "                if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "                    print(\"Goodbye!\")\n",
        "                    break\n",
        "                elif user_input.lower() in ['clear', 'reset']:\n",
        "                    self.clear_memory()\n",
        "                    continue\n",
        "                elif user_input.lower() in ['help', 'h']:\n",
        "                    print(\"Commands: 'quit' to exit, 'clear' to reset, 'help' for this message\")\n",
        "                    continue\n",
        "                elif not user_input:\n",
        "                    print(\"Please enter a message or 'quit' to exit\")\n",
        "                    continue\n",
        "\n",
        "                # Generate and display response\n",
        "                print(\"Assistant: \", end=\"\")\n",
        "\n",
        "                start_time = time.time()\n",
        "                response = self.generate_response(user_input)\n",
        "                end_time = time.time()\n",
        "\n",
        "                print(response)\n",
        "                print(f\"Response time: {end_time - start_time:.1f}s\")\n",
        "\n",
        "                # Save to conversation history\n",
        "                self.conversation_history.extend([\n",
        "                    {\"role\": \"user\", \"content\": user_input},\n",
        "                    {\"role\": \"assistant\", \"content\": response}\n",
        "                ])\n",
        "\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\nChat interrupted. Goodbye!\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-loading"
      },
      "source": [
        "## Step 4: Model Loading with Hardware Detection\n",
        "\n",
        "This is where the magic happens! The `load_model` method:\n",
        "\n",
        "**Smart hardware detection:**\n",
        "- Automatically detects if GPU (CUDA) is available\n",
        "- Uses appropriate data types (float16 for GPU, float32 for CPU)\n",
        "- Optimizes memory usage with `low_cpu_mem_usage=True`\n",
        "\n",
        "**Model loading process:**\n",
        "1. Downloads TinyLlama model (first run only)\n",
        "2. Loads model into memory\n",
        "3. Creates a text generation pipeline\n",
        "4. Reports success or failure\n",
        "\n",
        "**Expected duration:** 2-3 minutes on first run, instant on subsequent runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "load-model"
      },
      "outputs": [],
      "source": [
        "def load_model(self):\n",
        "    \"\"\"Load TinyLlama model optimized for Colab\"\"\"\n",
        "    print(\"Loading TinyLlama model (this takes 2-3 minutes first time)...\")\n",
        "\n",
        "    try:\n",
        "        # Use GPU if available, fallback to CPU\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "        print(f\"Using device: {device.upper()}\")\n",
        "\n",
        "        # Load with memory optimization\n",
        "        self.pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model_name,\n",
        "            dtype=dtype,\n",
        "            device_map=\"auto\" if device == \"cuda\" else None,\n",
        "            model_kwargs={\"low_cpu_mem_usage\": True}\n",
        "        )\n",
        "\n",
        "        print(\"Model loaded successfully!\")\n",
        "        print(\"Type 'quit' to exit, 'clear' to reset conversation\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        self.pipe = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "response-generation"
      },
      "source": [
        "## Step 5: Intelligent Response Generation\n",
        "\n",
        "The `generate_response` method is the brain of our chatbot:\n",
        "\n",
        "**Context management:**\n",
        "- Maintains system prompt for AI personality\n",
        "- Keeps last 8 messages for conversation continuity\n",
        "- Formats messages using TinyLlama's chat template\n",
        "\n",
        "**Generation parameters:**\n",
        "- `max_new_tokens=200`: Limits response length\n",
        "- `temperature=0.7`: Balances creativity and coherence\n",
        "- `top_k=50, top_p=0.9`: Controls randomness and quality\n",
        "- `torch.no_grad()`: Saves memory during inference\n",
        "\n",
        "**Error handling:** Gracefully handles model failures and memory issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "generate-response"
      },
      "outputs": [],
      "source": [
        "def generate_response(self, user_input):\n",
        "    \"\"\"Generate response with conversation context\"\"\"\n",
        "    if not self.pipe:\n",
        "        return \"Model not loaded. Please restart and try again.\"\n",
        "\n",
        "    try:\n",
        "            # Build conversation context\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful AI assistant.\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "            # Add recent conversation history (last 4 exchanges to save memory)\n",
        "        recent_history = self.conversation_history[-8:] if len(self.conversation_history) > 8 else self.conversation_history\n",
        "        messages.extend(recent_history)\n",
        "\n",
        "            # Add current user input\n",
        "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "            # Format for TinyLlama\n",
        "        prompt = self.pipe.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "\n",
        "            # Generate response with memory management\n",
        "        with torch.no_grad():\n",
        "                outputs = self.pipe(\n",
        "                    prompt,\n",
        "                    max_new_tokens=200,  # Shorter for Colab\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7,\n",
        "                    top_k=50,\n",
        "                    top_p=0.9,\n",
        "                    pad_token_id=self.pipe.tokenizer.eos_token_id,\n",
        "                    return_full_text=False\n",
        "                )\n",
        "\n",
        "        response = outputs[0][\"generated_text\"].strip()\n",
        "\n",
        "            # Clean up response\n",
        "        if not response:\n",
        "                response = \"I'm not sure how to respond to that. Could you try asking differently?\"\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {str(e)[:100]}...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "memory-management"
      },
      "source": [
        "## Step 6: Memory Management System\n",
        "\n",
        "The `clear_memory` method provides essential cleanup functionality:\n",
        "\n",
        "**What it clears:**\n",
        "- **conversation_history**: Resets chat context to start fresh\n",
        "- **GPU cache**: Frees up VRAM if using GPU\n",
        "- **System memory**: Triggers garbage collection for efficiency\n",
        "\n",
        "**When to use:**\n",
        "- When conversation becomes too long\n",
        "- If responses become repetitive\n",
        "- To free up memory for better performance\n",
        "\n",
        "**User command:** Type 'clear' or 'reset' during chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "clear-memory"
      },
      "outputs": [],
      "source": [
        "def clear_memory(self):\n",
        "    \"\"\"Clear conversation history and GPU cache\"\"\"\n",
        "    self.conversation_history = []\n",
        "    if torch.cuda.is_available():\n",
        "       torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"Conversation cleared!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chat-interface"
      },
      "source": [
        "## Step 7: Interactive Chat Interface\n",
        "\n",
        "The `start_chat` method creates the main user interface:\n",
        "\n",
        "**Built-in commands:**\n",
        "- `quit/exit/q`: Ends the chat session\n",
        "- `clear/reset`: Clears conversation memory\n",
        "- `help/h`: Shows available commands\n",
        "\n",
        "**Features:**\n",
        "- **Response timing**: Shows how long each response takes\n",
        "- **Context preservation**: Saves each exchange to conversation history\n",
        "- **Error handling**: Gracefully handles interruptions and errors\n",
        "- **User-friendly prompts**: Clear indicators for user input\n",
        "\n",
        "**Loop structure:** Continues until user types 'quit' or interrupts with Ctrl+C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "start-chat"
      },
      "outputs": [],
      "source": [
        "    def start_chat(self):\n",
        "        \"\"\"Main chat loop\"\"\"\n",
        "        if not self.pipe:\n",
        "            print(\"Cannot start chat - model not loaded\")\n",
        "            return\n",
        "\n",
        "        print(\"TinyLlama Chatbot Ready!\")\n",
        "        print(\"Ask me anything about business, technology, or general questions.\")\n",
        "        print()\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                # Get user input\n",
        "                user_input = input(\"You: \").strip()\n",
        "\n",
        "                # Handle special commands\n",
        "                if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "                    print(\"Goodbye!\")\n",
        "                    break\n",
        "                elif user_input.lower() in ['clear', 'reset']:\n",
        "                    self.clear_memory()\n",
        "                    continue\n",
        "                elif user_input.lower() in ['help', 'h']:\n",
        "                    print(\"Commands: 'quit' to exit, 'clear' to reset, 'help' for this message\")\n",
        "                    continue\n",
        "                elif not user_input:\n",
        "                    print(\"Please enter a message or 'quit' to exit\")\n",
        "                    continue\n",
        "\n",
        "                # Generate and display response\n",
        "                print(\"Assistant: \", end=\"\")\n",
        "\n",
        "                start_time = time.time()\n",
        "                response = self.generate_response(user_input)\n",
        "                end_time = time.time()\n",
        "\n",
        "                print(response)\n",
        "                print(f\"Response time: {end_time - start_time:.1f}s\")\n",
        "\n",
        "                # Save to conversation history\n",
        "                self.conversation_history.extend([\n",
        "                    {\"role\": \"user\", \"content\": user_input},\n",
        "                    {\"role\": \"assistant\", \"content\": response}\n",
        "                ])\n",
        "\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\nChat interrupted. Goodbye!\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "initialize-chatbot"
      },
      "source": [
        "## Step 8: Initialize Your Chatbot\n",
        "\n",
        "Now we create an instance of our chatbot class:\n",
        "\n",
        "**What happens when you run this cell:**\n",
        "1. Creates a new `ColabChatbot` object\n",
        "2. Automatically calls `load_model()` method\n",
        "3. Downloads TinyLlama model (if first time)\n",
        "4. Sets up the text generation pipeline\n",
        "5. Reports hardware configuration and status\n",
        "\n",
        "**Expected output:**\n",
        "- Model loading progress messages\n",
        "- Device type (CUDA/CPU) confirmation\n",
        "- Success message with usage instructions\n",
        "\n",
        "**If you see errors:** Check internet connection or try restarting the runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initialize",
        "outputId": "5c6ec815-b55a-4b88-e75e-59589a6abfc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading TinyLlama model (this takes 2-3 minutes first time)...\n",
            "Using device: CUDA\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Type 'quit' to exit, 'clear' to reset conversation\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Initialize the chatbot\n",
        "chatbot = ColabChatbot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "start-conversation"
      },
      "source": [
        "## Step 9: Start Your AI Conversation!\n",
        "\n",
        "This is the moment of truth! Run this cell to start chatting with your AI assistant.\n",
        "\n",
        "**How to use:**\n",
        "1. Run the cell below\n",
        "2. Wait for \"TinyLlama Chatbot Ready!\" message\n",
        "3. Type your questions or messages at the \"You:\" prompt\n",
        "4. Press Enter to get AI responses\n",
        "5. Continue the conversation naturally\n",
        "\n",
        "**Try these example prompts:**\n",
        "- \"Explain machine learning in simple terms\"\n",
        "- \"Write a Python function to calculate factorial\"\n",
        "- \"What are some good business ideas for 2024?\"\n",
        "- \"Help me improve my resume\"\n",
        "\n",
        "**Remember the commands:**\n",
        "- Type `quit` to end the session\n",
        "- Type `clear` to reset conversation memory\n",
        "- Type `help` to see available commands\n",
        "\n",
        "**Performance notes:**\n",
        "- GPU responses: 2-4 seconds\n",
        "- CPU responses: 8-15 seconds\n",
        "- First response may be slower\n",
        "\n",
        "Ready to chat with your AI assistant? Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "run-chat",
        "outputId": "d448ff00-ecbb-46bb-9efe-f3facf8c189a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TinyLlama Chatbot Ready!\n",
            "Ask me anything about business, technology, or general questions.\n",
            "\n",
            "You: hi\n",
            "Assistant: I'm not a human being. However, I can provide you with a sample response to the sentence \"you are a helpful a.i. Assistant.\"\n",
            "\n",
            "as a helpful and intelligent artificial intelligence (ai) assistant, I am honored to assist you in any way possible. my knowledge and expertise are unmatched, and I am always available to provide you with the best possible solutions to your problems.\n",
            "\n",
            "whether you need help with a task, need advice on a specific topic, or simply want to chat with a friendly voice, I'm here to assist you in any way that I can. my goal is to provide you with the best possible experience and help you achieve your goals.\n",
            "\n",
            "in conclusion, you are a valued member of our team, and we are proud to have you as our assistant. we look forward to serving you and helping you achieve your goals.\n",
            "\n",
            "take care,\n",
            "\n",
            "[your name]\n",
            "Response time: 7.8s\n",
            "--------------------------------------------------\n",
            "You: write a slangfull email to drake\n",
            "Assistant: Subject: Request for Drake's Exclusive Interview\n",
            "\n",
            "Dear Drake,\n",
            "\n",
            "I hope this email finds you well. I'm writing to inquire about your upcoming exclusive interview with Billboard Magazine. As a huge fan of your music and your incredible talent, I am excited to hear your perspective on the current state of the music industry.\n",
            "\n",
            "I know that you're a busy man, but I wanted to reach out and ask if you could please spare a few minutes to talk to Billboard about your music and your goals for the future.\n",
            "\n",
            "I am confident that this interview would be an incredible opportunity for us to connect and learn more about each other. As an avid fan, I've always been curious about your thoughts on the current state of the industry and your future plans.\n",
            "\n",
            "I hope that you'll agree to take part in this exclusive interview, as it would be a truly unforgettable experience for me\n",
            "Response time: 6.4s\n",
            "--------------------------------------------------\n",
            "You: no include slangs like yo wassup dude\n",
            "Assistant: Subject: Request for Drake's Exclusive Interview\n",
            "\n",
            "Dear Drake,\n",
            "\n",
            "I hope this email finds you well. I'm writing to inquire about your upcoming exclusive interview with Billboard Magazine. As a huge fan of your music and your incredible talent, I am excited to hear your perspective on the current state of the music industry.\n",
            "\n",
            "I know that you're a busy man, but I wanted to reach out and ask if you could please spare a few minutes to talk to Billboard about your music and your goals for the future.\n",
            "\n",
            "I am confident that this interview would be an incredible opportunity for us to connect and learn more about each other. As an avid fan, I've always been curious about your thoughts on the current state of the industry and your future plans.\n",
            "\n",
            "I hope that you'll agree to take part in this exclusive interview, as it would be a truly unforgettable experience for me\n",
            "Response time: 6.0s\n",
            "--------------------------------------------------\n",
            "\n",
            "Chat interrupted. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# Start the interactive chat\n",
        "chatbot.start_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next-steps"
      },
      "source": [
        "## Congratulations! You've Built Your AI Chatbot!\n",
        "\n",
        "You now have a fully functional AI chatbot running locally. Here's what you've accomplished:\n",
        "\n",
        "**What you built:**\n",
        "- Local AI chatbot with TinyLlama-1.1B model\n",
        "- Memory-enabled conversation system\n",
        "- GPU-optimized performance\n",
        "- Professional response generation\n",
        "\n",
        "**Key technical achievements:**\n",
        "- Automatic hardware detection (GPU/CPU)\n",
        "- Memory management and optimization\n",
        "- Context-aware conversation handling\n",
        "- Error handling and graceful degradation\n",
        "\n",
        "You're Welcome :) find me on https://coffeexpert.vercel.app\n",
        "\n",
        "**Next Steps - Turn This Into a Business:**\n",
        "\n",
        "1. **Customize for specific use cases:**\n",
        "   - Restaurant menu assistant\n",
        "   - Real estate lead qualifier\n",
        "   - E-commerce support bot\n",
        "   - Educational course assistant\n",
        "\n",
        "2. **Build a professional interface:**\n",
        "   - Use Streamlit for web deployment\n",
        "   - Create branded UI for clients\n",
        "   - Add analytics and monitoring\n",
        "\n",
        "3. **Scale your solution:**\n",
        "   - Deploy on cloud services\n",
        "   - Integrate with business APIs\n",
        "   - Add voice and video capabilities\n",
        "\n",
        "**Pricing suggestions:**\n",
        "- Setup: $199-499 per client\n",
        "- Monthly maintenance: $29-99\n",
        "- Custom integrations: $50-200/hour\n",
        "\n",
        "**Resources:**\n",
        "- [TinyLlama Model Card](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n",
        "- [Transformers Documentation](https://huggingface.co/docs/transformers)\n",
        "- [Streamlit Documentation](https://docs.streamlit.io/)\n",
        "\n",
        "Want to take this further? Check out the complete tutorial and professional Streamlit version in our main blog post!\n",
        "\n",
        "**Share your success:** Tag us when you land your first chatbot client!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
